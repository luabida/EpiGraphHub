{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bcea2d-a6f2-401c-a650-b30213588898",
   "metadata": {},
   "source": [
    "This notebooks will describe the steps I took to migrate Superset internal database from Sqlite3 to PostgreSQL-15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741ed69-eaa1-414f-9da2-aaf7d6b612a2",
   "metadata": {},
   "source": [
    "Firstly, for safety, I'll create a copy of the sqlite3 database. The location of the database file can be found in the `superset_config.py` assigned to the variable `SQLALCHEMY_DATABASE_URI`:\n",
    "\n",
    "```sh\n",
    "cp /opt/data/superset/superset.db /opt/data/superset/superset.db.bak\n",
    "```\n",
    "\n",
    "Then, I'll [rsync](https://linux.die.net/man/1/rsync) the copied file to my machine, so I can work with it locally:\n",
    "\n",
    "```sh\n",
    "rsync -av ***:/opt/data/superset/superset.db.bak /home/bida/superset/superset.db\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073443e1-272b-4472-a23f-5c4a765e197d",
   "metadata": {},
   "source": [
    "Next step is to locally build the Superset database that will be populated with the Sqlite tables. To do so, the new database URI have to replace the sqlite3 database path in the variable `SQLALCHEMY_DATABASE_URI`, with the correct user permissions beforehand. The configuration below is the credentials to the postgres container in my machine, the IP and port of the database are configured in the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd18f178-6031-4567-8bd1-a887d188bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "DB_HOST = \"172.27.0.2\"\n",
    "DB_PORT = 15432\n",
    "DB_USER = \"dev_admin\"\n",
    "DB_PASS = \"admin\"\n",
    "DB_DATABASE = \"dev_superset\"\n",
    "SQLALCHEMY_DATABASE_URI = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_DATABASE}\"\n",
    "\n",
    "psql_con = create_engine(SQLALCHEMY_DATABASE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8c724-ef6a-4f25-bd6d-5e7bde03d6a3",
   "metadata": {},
   "source": [
    "### Preparing PostgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520975a-60e5-43f7-aaf5-9b0137e933f9",
   "metadata": {},
   "source": [
    "When building the Superset container, if the postgres user and database are properly configured, the database structure will be generated with the default data in it. To ensure there won't be conflicting data, it's a good idea to clean the database before start the migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb1a0e7-7074-4d6f-a359-3a8c297afbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sqlite_db = '/home/bida/superset/superset.db'\n",
    "sqlite_con = sqlite3.connect(sqlite_db)\n",
    "cursor = sqlite_con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "sqlite_tables = cursor.fetchall()\n",
    "sqlite_con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01572cc-8e6e-4c45-b1ee-85b704fd5a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "sqlite_tables = list(chain(*sqlite_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed84259-02b3-40c3-b7e4-f8b22c5e46a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "inspector = inspect(psql_con)\n",
    "\n",
    "psql_tables = inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a163d02-d97f-4a2f-bc39-a4e2b2716b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "superset_tables = list(set(psql_tables).intersection(sqlite_tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cefef-ad51-4134-a332-d2ecfa690ac7",
   "metadata": {},
   "source": [
    "#### WARNING - THIS WILL DELETE ALL DATA IN SUPERSET'S POSTGRES DATABASE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e4d72e-6fd6-445b-92c4-f93437ba856b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269513/964981679.py:4: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  conn.execute(sql)\n"
     ]
    }
   ],
   "source": [
    "with psql_con.connect() as conn:\n",
    "    for table in superset_tables:\n",
    "        sql = f\"ALTER TABLE {table} DISABLE TRIGGER ALL; DELETE FROM {table}\"\n",
    "        conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d655ab-28c3-491b-9ffd-ae8daee837a6",
   "metadata": {},
   "source": [
    "### Tables - Topological Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6a558-4876-4fcb-884b-43916f0b8b97",
   "metadata": {},
   "source": [
    "Now it's time to create the tables hierarchy, respecting the foreign keys from each table. \n",
    "\n",
    "The rules are:\n",
    "- Tables that don't contain foreign keys will have to be inserted first\n",
    "- A table can't be inserted until all of its foreign key tables have been inserted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930800d4-4b11-47f9-8466-a5fbbe6d96c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fkeys = dict()\n",
    "for table in superset_tables:\n",
    "    fkey_dicts = inspector.get_foreign_keys(table)\n",
    "    fkeys[table] = set()\n",
    "    for fk in fkey_dicts:\n",
    "        if table == fk['referred_table']:\n",
    "            continue\n",
    "        fkeys[table].add(fk['referred_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d605891c-e1a5-496d-991d-1f25a7a8eb36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clusters', {'ab_user'})\n",
      "('ab_role', set())\n",
      "('url', {'ab_user'})\n",
      "('slice_email_schedules', {'slices', 'ab_user'})\n",
      "('rls_filter_tables', {'tables', 'row_level_security_filters'})\n",
      "('css_templates', {'ab_user'})\n"
     ]
    }
   ],
   "source": [
    "# Result (table, {foreign_keys}):\n",
    "i = 0\n",
    "for fk in fkeys.items():\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(fk)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0f8c4-8973-4e08-96dd-84dc266f903c",
   "metadata": {},
   "source": [
    "Now that we have calculated which tables are foreign keys to each table, we can use [graphlib](https://docs.python.org/3/library/graphlib.html) package in Python, that has a [Topological sort](https://en.wikipedia.org/wiki/Topological_sorting) algorithm. The `TopologicalSorter` will calculate that hierarchical result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d9bb16-18b2-494a-9000-fdf75e1be211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphlib import TopologicalSorter\n",
    "ts = TopologicalSorter(fkeys)\n",
    "tables_order = tuple(ts.static_order())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd5dbf9-356b-40d9-8628-bc655ba6cca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ab_user', 'ab_role', 'ab_permission', 'ab_view_menu', 'keyvalue', 'ab_register_user', 'cache_keys', 'alembic_version', 'clusters', 'url', 'css_templates', 'dynamic_plugin', 'tag', 'slices', 'key_value', 'annotation_layer', 'logs', 'access_request', 'dashboards', 'dbs', 'row_level_security_filters', 'favstar', 'ab_user_role', 'ab_permission_view', 'datasources', 'tagged_object', 'slice_email_schedules', 'slice_user', 'annotation', 'embedded_dashboards', 'dashboard_user', 'user_attribute', 'dashboard_email_schedules', 'dashboard_slices', 'filter_sets', 'dashboard_roles', 'saved_query', 'report_schedule', 'query', 'alerts', 'tables', 'sl_datasets', 'sl_tables', 'rls_filter_roles', 'ab_permission_view_role', 'metrics', 'druiddatasource_user', 'columns', 'report_execution_log', 'report_recipient', 'report_schedule_user', 'tab_state', 'alert_owner', 'sql_observations', 'alert_logs', 'rls_filter_tables', 'sql_metrics', 'table_columns', 'sqlatable_user', 'table_schema')\n"
     ]
    }
   ],
   "source": [
    "print(tables_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184cfb8-7fca-49e7-bbdb-09ed1dff045a",
   "metadata": {},
   "source": [
    "### Populating the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e39243-94e5-453d-811d-aaa812b6de75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy.dtypes import BoolDType\n",
    "from uuid import UUID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b331642-a4f5-4770-b3d6-b6f2e1878b86",
   "metadata": {},
   "source": [
    "Some columns will require data parsing in its values. For instance, the boolean columns in the sqlite tables are represented with 0 and 1s (int64), while in the postgres table, they are boolean fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471945fb-678a-49bb-b67e-25b893e1f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_to_df(table: str) -> pd.DataFrame:\n",
    "    sqlite_con = sqlite3.connect(sqlite_db)\n",
    "    sqlite_df = pd.read_sql(f\"SELECT * FROM {table}\", sqlite_con)\n",
    "    sqlite_con.close()\n",
    "    \n",
    "    sqlite_df = sqlite_df.convert_dtypes()\n",
    "    psql_df = pd.read_sql_table(table, psql_con) # Empty table\n",
    "    \n",
    "    for column, dtype in psql_df.dtypes.items():\n",
    "        if isinstance(dtype, BoolDType):\n",
    "            sqlite_df[column] = sqlite_df[column].apply(lambda x: bool(x) if isinstance(x, int) else x)\n",
    "        \n",
    "        if column == 'uuid':\n",
    "            sqlite_df[column] = sqlite_df[column].apply(lambda x: UUID(bytes=x))\n",
    "        \n",
    "    # https://github.com/apache/superset/pull/21284/files#diff-633d4f34919db8ab7db651657ebf8a578f437ca1f95d48c4b27c6ccd83c54640R36\n",
    "    if table == 'dbs':\n",
    "        sqlite_df = sqlite_df.drop(columns=['allow_multi_schema_metadata_fetch'])\n",
    "    \n",
    "    return sqlite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668d38b5-225c-4e5f-8eae-451505770b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 inserted on ab_user\n",
      "10 inserted on ab_role\n",
      "107 inserted on ab_permission\n",
      "277 inserted on ab_view_menu\n",
      "0 inserted on keyvalue\n",
      "9 inserted on ab_register_user\n",
      "492 inserted on cache_keys\n",
      "1 inserted on alembic_version\n",
      "0 inserted on clusters\n",
      "42 inserted on url\n",
      "2 inserted on css_templates\n",
      "0 inserted on dynamic_plugin\n",
      "6 inserted on tag\n",
      "138 inserted on slices\n",
      "2923 inserted on key_value\n",
      "5 inserted on annotation_layer\n",
      "0 inserted on access_request\n",
      "65 inserted on dashboards\n",
      "7 inserted on dbs\n",
      "0 inserted on row_level_security_filters\n",
      "27 inserted on favstar\n",
      "101 inserted on ab_user_role\n",
      "440 inserted on ab_permission_view\n",
      "0 inserted on datasources\n",
      "36 inserted on tagged_object\n",
      "0 inserted on slice_email_schedules\n",
      "146 inserted on slice_user\n",
      "11 inserted on annotation\n",
      "0 inserted on embedded_dashboards\n",
      "66 inserted on dashboard_user\n",
      "0 inserted on user_attribute\n",
      "0 inserted on dashboard_email_schedules\n",
      "87 inserted on dashboard_slices\n",
      "0 inserted on filter_sets\n",
      "0 inserted on dashboard_roles\n",
      "18 inserted on saved_query\n",
      "0 inserted on report_schedule\n",
      "1315 inserted on query\n",
      "0 inserted on alerts\n",
      "127 inserted on tables\n",
      "0 inserted on sl_datasets\n",
      "0 inserted on sl_tables\n",
      "0 inserted on rls_filter_roles\n",
      "1092 inserted on ab_permission_view_role\n",
      "0 inserted on metrics\n",
      "0 inserted on druiddatasource_user\n",
      "0 inserted on columns\n",
      "0 inserted on report_execution_log\n",
      "0 inserted on report_recipient\n",
      "0 inserted on report_schedule_user\n",
      "49 inserted on tab_state\n",
      "0 inserted on alert_owner\n",
      "0 inserted on sql_observations\n",
      "0 inserted on alert_logs\n",
      "0 inserted on rls_filter_tables\n",
      "127 inserted on sql_metrics\n",
      "3471 inserted on table_columns\n",
      "127 inserted on sqlatable_user\n",
      "26 inserted on table_schema\n"
     ]
    }
   ],
   "source": [
    "with psql_con.connect() as conn:\n",
    "    for table in tables_order:\n",
    "        if table == \"logs\":\n",
    "            continue # ignore logs table\n",
    "            \n",
    "        df = parse_to_df(table)\n",
    "        df.to_sql(table, conn, index=False, if_exists=\"append\")\n",
    "        \n",
    "        print(f\"{len(df)} inserted on {table}\")\n",
    "        \n",
    "        if \"id\" in df.columns:\n",
    "            max_id = 0 if not str(df.id.max()).isnumeric() else df.id.max()\n",
    "            sql = (\n",
    "                f\"ALTER TABLE {table} ALTER COLUMN id SET DEFAULT nextval('{table}_id_seq'), ENABLE TRIGGER ALL; \"\n",
    "                f\"ALTER SEQUENCE {table}_id_seq RESTART WITH {max_id + 1}; \"\n",
    "                \n",
    "            )\n",
    "            conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7d69a-396f-43f0-910b-9d650fd3241c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Checking if tables have same lengh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d477a8ab-edb8-4593-99d0-589db8e483cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psql_len = 0\n",
    "sqlite_len = 0\n",
    "for table in tables_order:\n",
    "    if table == \"logs\":\n",
    "        continue\n",
    "        \n",
    "    psql_len += len(pd.read_sql_table(table, psql_con))\n",
    "    \n",
    "    sqlite_con = sqlite3.connect(sqlite_db)\n",
    "    sqlite_len += len(pd.read_sql(f\"SELECT * FROM {table}\", sqlite_con))\n",
    "    sqlite_con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019a28b6-dca5-4917-b99d-6afdaef2de28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(psql_len == sqlite_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ad37b-0c7b-443f-9d6b-fecdc50d46ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
